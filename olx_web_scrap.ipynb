{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5a04c5-17ee-4008-b01d-1c5bcf6ff405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad31258a-0367-4357-a15f-463fec4eb9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a request and associa the response in the variable 'response'\n",
    "# print(response) give me only \"<Response [200]>\"\n",
    "# but 'response' too receive all content in the site atravês do 'response.content'\n",
    "# the content que foi recebido ainda não e html, é somente texto\n",
    "def make_request(url):\n",
    "    headers = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac179fc-2fb4-4408-ab2e-0feecc0da739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'soup' receive a text (returned for 'make_request()') and converto to html\n",
    "# 'anuncios' receive all div content in the soup\n",
    "def parse_html(html, tag, a_class):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    anuncios = soup.find_all(tag, class_= a_class)\n",
    "    return anuncios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0450052-28e0-44c0-9ed9-bf882c9058f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_individual_link(anuncio):\n",
    "    link = anuncio.find('a', class_='gIhjul')['href']\n",
    "    return link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a4cfc3-e1a9-4a7c-a3eb-65fbb8b87651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapy(modelo, carro, regiao, qts_paginas):\n",
    "\n",
    "    url = f'https://www.olx.com.br/autos-e-pecas/carros-vans-e-utilitarios/{modelo}/{carro}/estado-sp/{regiao}'\n",
    "    site = make_request(url)\n",
    "    anuncios = parse_html(site.content)\n",
    "\n",
    "\n",
    "    for anuncio in anuncios:\n",
    "        link = extract_car_info(anuncio, 'a', 'etGiBL')\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2da3e1-197a-45d7-b9e2-806a1ab8f596",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelo = 'vw-volkswagen'\n",
    "carro = 'gol'\n",
    "regioes = [['sao-paulo-e-regiao', round(3440/50)],\n",
    "            ['vale-do-paraiba-e-litoral-norte', round(894/50)],\n",
    "            ['baixada-santista-e-litoral-sul', round(306/50)],\n",
    "            ['regiao-de-bauru-e-marilia', round(370/50)],\n",
    "            ['regiao-de-sorocaba', round(778/50)],\n",
    "            ['regiao-de-ribeirao-preto', round(672/50)],\n",
    "            ['regiao-de-sao-jose-do-rio-preto', round(617/50)],\n",
    "            ['regiao-de-presidente-prudente', round(262/50)],\n",
    "            ['grande-campinas', round(1056/50)]]\n",
    "\n",
    "#first_page_link = f'https://www.olx.com.br/autos-e-pecas/carros-vans-e-utilitarios/{modelo}/{carro}/estado-sp/{regiao}'\n",
    "#other_pages_link = f'https://www.olx.com.br/autos-e-pecas/carros-vans-e-utilitarios/{modelo}/{carro}/estado-sp/{regiao}?o={i}'\n",
    "#tag_main_page = 'div'\n",
    "#class_main_page = 'hrShZb'\n",
    "   \n",
    "#tag_link = 'a'\n",
    "#a_class_link = 'etGiBL'\n",
    "\n",
    "#data = []\n",
    "\n",
    "for regiao in regioes:\n",
    "    \n",
    "    url = f'https://www.olx.com.br/autos-e-pecas/carros-vans-e-utilitarios/{modelo}/{carro}/estado-sp/{regiao[0]}'\n",
    "    \n",
    "    site = make_request(url)\n",
    "\n",
    "    tag = 'div'\n",
    "    a_class = 'hrShZb'\n",
    "    anuncios = parse_html(site.content, tag, a_class)\n",
    "\n",
    "    an = 1\n",
    "    for anuncio in anuncios:\n",
    "        print(f'ANÚNCIO {an}')\n",
    "        link = anuncio.find('a', class_='gIhjul')['href']\n",
    "        km = anuncio.find('span').get_text()\n",
    "        print(link)\n",
    "        \n",
    "        site = make_request(link)\n",
    "        tag = 'div'\n",
    "        a_class = 'eCUDNu'\n",
    "        infos = parse_html(site.content, tag, a_class)\n",
    "        \n",
    "        for info in infos:\n",
    "\n",
    "            nome = info.find('h1', class_='bYQcLm')\n",
    "            if nome is not None:\n",
    "                nome = nome.get_text()\n",
    "                print(nome)\n",
    "            else:\n",
    "                print(nome)\n",
    "\n",
    "            preco_atual = info.find('h2', class_='ad__sc-1leoitd-0 bJHaGt sc-hSdWYo dDGSHH')\n",
    "            if preco_atual is not None:\n",
    "                preco_atual = preco_atual.get_text()\n",
    "                print(preco_atual)\n",
    "            else:\n",
    "                print(preco_atual)\n",
    "            \n",
    "            preco_anunciado = info.find('span', class_='ad__sc-1leoitd-1 bMUiTp sc-hSdWYo htqcWR')\n",
    "            if preco_anunciado is not None:\n",
    "                preco_anunciado = preco_anunciado.get_text()\n",
    "                print(preco_anunciado)\n",
    "            else:\n",
    "                print(preco_anunciado)\n",
    "                \n",
    "            print(km)\n",
    "                \n",
    "            title = info.findAll('a', class_='sc-EHOje lePqYm')\n",
    "            model = title[2].get_text()\n",
    "            ano = title[4].get_text()\n",
    "            combustivel = title[5].get_text()\n",
    "            print(model)\n",
    "            print(ano)\n",
    "            print(combustivel)\n",
    "        an += 1\n",
    "            \n",
    "#print(data)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabb8981-9011-48a2-8eea-4a7399733c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = 'vw-volkswagen'\n",
    "carro = 'gol'\n",
    "regioes = [['sao-paulo-e-regiao', round(3440/50)],\n",
    "            ['vale-do-paraiba-e-litoral-norte', round(894/50)],\n",
    "            ['baixada-santista-e-litoral-sul', round(306/50)],\n",
    "            ['regiao-de-bauru-e-marilia', round(370/50)],\n",
    "            ['regiao-de-sorocaba', round(778/50)],\n",
    "            ['regiao-de-ribeirao-preto', round(672/50)],\n",
    "            ['regiao-de-sao-jose-do-rio-preto', round(617/50)],\n",
    "            ['regiao-de-presidente-prudente', round(262/50)],\n",
    "            ['grande-campinas', round(1056/50)]]\n",
    "\n",
    "for regiao in regioes:\n",
    "\n",
    "    #requisição na página inicial\n",
    "    url = f'https://www.olx.com.br/autos-e-pecas/carros-vans-e-utilitarios/{modelo}/{carro}/estado-sp/{regiao[0]}'\n",
    "    print(url)\n",
    "    site = make_request(url)\n",
    "\n",
    "    #tag da página inicial\n",
    "    tag = 'div'\n",
    "    a_class = 'hrShZb'\n",
    "\n",
    "    #retorna html com todos os anúncios\n",
    "    anuncios = parse_html(site.content, tag, a_class)\n",
    "\n",
    "    #pega o link, km e ano de cada anuncio\n",
    "    for anuncio in anuncios:\n",
    "        link = anuncio.find('a', class_='gIhjul')['href']\n",
    "        info = anuncio.findAll('span')\n",
    "        km = info[0].get_text()\n",
    "        ano = info[1].get_text()\n",
    "        print(link)\n",
    "        print(km)\n",
    "        print(ano)\n",
    "    \n",
    "    \n",
    "        #requisição para cada link individual\n",
    "        site = make_request(link)\n",
    "        #tag com as infos de cada anúncio\n",
    "        tag = 'div'\n",
    "        a_class = 'eCUDNu'\n",
    "        #retorna html com todas as informaçõe de cada anúncio\n",
    "        infos = parse_html(site.content, tag, a_class)\n",
    "        for info in infos:\n",
    "            print(regiao[0])\n",
    "            nome = info.find('h1', class_='bYQcLm')\n",
    "            if nome is not None:\n",
    "                nome = nome.get_text()\n",
    "                print(nome)\n",
    "            else:\n",
    "                print(nome)\n",
    "            \n",
    "            preco_atual = info.find('h2', class_='ad__sc-1leoitd-0 bJHaGt sc-hSdWYo dDGSHH')\n",
    "            if preco_atual is not None:\n",
    "                preco_atual = preco_atual.get_text()\n",
    "                print(preco_atual)\n",
    "            else:\n",
    "                print(preco_atual)\n",
    "            \n",
    "            preco_anunciado = info.find('span', class_='ad__sc-1leoitd-1 bMUiTp sc-hSdWYo htqcWR')\n",
    "            if preco_anunciado is not None:\n",
    "                preco_anunciado = preco_anunciado.get_text()\n",
    "                print(preco_anunciado)\n",
    "            else:\n",
    "                print(preco_anunciado)\n",
    "                pass\n",
    "                \n",
    "            model = info.find('a', class_='sc-EHOje lePqYm')\n",
    "            if model is not None:\n",
    "                model = model.get_text()\n",
    "                print(model)\n",
    "            else:\n",
    "                print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee236a2-5b95-403e-9f8e-2f498e254a5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
